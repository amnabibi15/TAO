{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "# import cv2\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\", context=\"paper\")\n",
    "from cycler import cycler\n",
    "import os, sys\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import combinations\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO as _BytesIO\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.metrics import *\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "pd.options.display.max_columns = None\n",
    "def printm(s): return display(Markdown(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_weekly_stress_score(week_context_attributes):\n",
    "    \"\"\"\n",
    "    Return stress score for the week\n",
    "    :param week_context_attributes: How days context looks like\n",
    "    :return: weekly stress score based on FSM\n",
    "    \"\"\"\n",
    "    stress_score = 0.\n",
    "    total_days_data_in_week = len(week_context_attributes.keys())\n",
    "\n",
    "    # Condition 1: Context of exercising and housework existing in 57% of week: -1 stress score\n",
    "    c1_exercise_count, c1_housework_count = 0.,0.\n",
    "    for day in week_context_attributes.keys():\n",
    "        if 'Exercising' in week_context_attributes[day].context.values:\n",
    "            c1_exercise_count +=1\n",
    "        if 'HouseWork' in week_context_attributes[day].context.values:\n",
    "            c1_housework_count+=1\n",
    "\n",
    "    if ((c1_exercise_count/total_days_data_in_week) >= 0.57) and ((c1_housework_count/total_days_data_in_week) >= 0.57):\n",
    "        stress_score -= 1\n",
    "\n",
    "    # Condition 2a: No context of exercising for more than 57% week(4 days if full week available): +0.5 stress score\n",
    "    # Condition 2b: No context of housework  for more than 4 days(57% week): +0.5 stress score\n",
    "    c2a_exercise_count = c1_exercise_count\n",
    "    c2b_housework_count = c1_housework_count\n",
    "\n",
    "    if (c2a_exercise_count/total_days_data_in_week) < 0.57:\n",
    "        stress_score += 0.5\n",
    "    if (c2b_housework_count/total_days_data_in_week) < 0.57:\n",
    "        stress_score += 0.5\n",
    "\n",
    "    # Condition 3: context of commute for more than an hour(on average) in a day everyday: +0.5 stress score\n",
    "    c3_commute_hours = []\n",
    "    for day in week_context_attributes:\n",
    "        if 'Commuting' not in week_context_attributes[day].context.values:\n",
    "            c3_commute_hours.append(0.)\n",
    "        else:\n",
    "            day_commute_values = week_context_attributes[day][week_context_attributes[day].context=='Commuting']\n",
    "            day_commute_values['period'] = day_commute_values['end'] - day_commute_values['start']\n",
    "            c3_commute_hours.append(day_commute_values['period'].sum())\n",
    "\n",
    "    if np.mean(c3_commute_hours) > (60*60): # more than 1 hours of average commute\n",
    "        stress_score +=0.5\n",
    "\n",
    "\n",
    "    # condition 4a: context of sleep for less than 6 hours on more than 3 days(42% week): +1 stress score\n",
    "    # condition 4b: context of sleep for less more than 8 hours on an average: -0.5 stress score\n",
    "    c4_sleep_hours = []\n",
    "    for day in week_context_attributes:\n",
    "        if 'Sleeping' not in week_context_attributes[day].context.values:\n",
    "            c4_sleep_hours.append(0.)\n",
    "        else:\n",
    "            day_sleep_values = week_context_attributes[day][week_context_attributes[day].context == 'Sleeping']\n",
    "            day_sleep_values['period'] = day_sleep_values['end'] - day_sleep_values['start']\n",
    "            c4_sleep_hours.append(day_sleep_values['period'].sum())\n",
    "\n",
    "    if np.max(sorted(c4_sleep_hours)[:3]) < (6 * 60 * 60):  # 4a.less than 6 hours of sleep for 3 days\n",
    "        stress_score += 1\n",
    "\n",
    "    if np.mean(c4_sleep_hours) > (8 * 60 * 60):  # 4b. more than 8 hours of sleep\n",
    "        stress_score -= 0.5\n",
    "\n",
    "    # condition 5: (More than 4 days(57%) of Exercising context for more than half an hour) &\n",
    "    # (Different working hours (>40% difference) across days): +0.5 stress score\n",
    "    c5_exercising_count = c1_exercise_count\n",
    "    c5_exercising_hours = []\n",
    "    for day in week_context_attributes:\n",
    "        if 'Exercising' in week_context_attributes[day].context.values:\n",
    "            day_exercise_values = week_context_attributes[day][week_context_attributes[day].context == 'Exercising']\n",
    "            day_exercise_values['period'] = day_exercise_values['end'] - day_exercise_values['start']\n",
    "            c5_exercising_hours.append(day_exercise_values['period'].sum())\n",
    "    c5_exercise_cond = False\n",
    "    if ((c5_exercising_count/total_days_data_in_week) >= 0.57) and (np.min(c5_exercising_hours) > (0.5*60*60)):\n",
    "        c5_exercise_cond = True\n",
    "\n",
    "    c5_working_periods = {}\n",
    "    for day in week_context_attributes:\n",
    "        if 'OfficeWork' in week_context_attributes[day].context.values:\n",
    "            day_work_values = week_context_attributes[day][week_context_attributes[day].context == 'OfficeWork']\n",
    "            day_work_values['start_hour'] = pd.to_datetime(day_work_values['start'],unit='s').dt.hour\n",
    "            day_work_values['end_hour'] = pd.to_datetime(day_work_values['end'], unit='s').dt.hour\n",
    "            c5_working_periods[day] = np.zeros(24)\n",
    "            for idx, row in day_work_values.iterrows():\n",
    "                c5_working_periods[day][row['start_hour']:row['end_hour']+1] = 1\n",
    "\n",
    "    c5_hour_differences= [0.]\n",
    "    for dayA in c5_working_periods.keys():\n",
    "        for dayB in c5_working_periods.keys():\n",
    "            if not (dayA==dayB):\n",
    "                working_hoursA = c5_working_periods[dayA]\n",
    "                working_hoursB = c5_working_periods[dayB]\n",
    "                diff_hours = np.sum(working_hoursA!=working_hoursB)\n",
    "                same_hours = np.sum(np.logical_and(working_hoursA==working_hoursB,working_hoursA))\n",
    "                if same_hours > 0.:\n",
    "                    c5_hour_differences.append(diff_hours / same_hours)\n",
    "                else:\n",
    "                    c5_hour_differences.append(0.)\n",
    "\n",
    "    c5_working_cond = False\n",
    "    if np.max(c5_hour_differences) > 0.4:\n",
    "        c5_working_cond = True\n",
    "\n",
    "    if (c5_working_cond) & (c5_exercise_cond):\n",
    "        stress_score +=0.5\n",
    "\n",
    "    # condition 6: (No context of exercising) & (Different working hours across days): +1 stress score\n",
    "    c6_exercise_count=  c1_exercise_count\n",
    "    c6_working_hour_diff = c5_hour_differences\n",
    "    if (c6_exercise_count==0.) & (np.mean(c5_hour_differences) > 0.4):\n",
    "        stress_score += 1\n",
    "\n",
    "    # condition 7: (More than 4 days of Exercising context for more than half an hour) &\n",
    "    # (No Different working hours (>40% difference)  across days): -1 stress score\n",
    "    c7_exercise_cond = c5_exercise_cond\n",
    "    c7_hour_differences = c5_hour_differences\n",
    "\n",
    "    if (c7_exercise_cond) & (np.max(c7_hour_differences) < 0.4):\n",
    "        stress_score -= 1.\n",
    "\n",
    "    # condition 8: (More than 10 hrs of works in any day) OR (50+ hours per week): +1 stress score\n",
    "    c8_work_hours = []\n",
    "    for day in week_context_attributes:\n",
    "        if 'OfficeWork' in week_context_attributes[day].context.values:\n",
    "            day_work_values = week_context_attributes[day][week_context_attributes[day].context == 'OfficeWork']\n",
    "            day_work_values['period'] = day_work_values['end'] - day_work_values['start']\n",
    "            c8_work_hours.append(day_work_values['period'].sum())\n",
    "        else:\n",
    "            c8_work_hours.append(0.)\n",
    "\n",
    "    if (np.max(c8_work_hours) > (10 * 60 * 60)) or (np.sum(c8_work_hours) > (50 * 60 * 60)):\n",
    "        stress_score += 1\n",
    "\n",
    "    # condition 9a: (More than 40% shift in working hours in max gap): +1 stress score\n",
    "    # condition 9b: (More than 40% shift in working hours in 2 or more consecutive days): +1 stress score\n",
    "    # Not modeling due to over complexity\n",
    "\n",
    "    # condition 10a: More than 15 mins exercising every day: -0.5 stress score\n",
    "    # condition 10b: More than 1 hour housework every day: -0.5 stress score\n",
    "    c10a_exercising_hours = []\n",
    "    for day in week_context_attributes:\n",
    "        if 'Exercising' in week_context_attributes[day].context.values:\n",
    "            day_exercise_values = week_context_attributes[day][week_context_attributes[day].context == 'Exercising']\n",
    "            day_exercise_values['period'] = day_exercise_values['end'] - day_exercise_values['start']\n",
    "            c10a_exercising_hours.append(day_exercise_values['period'].sum())\n",
    "        else:\n",
    "            c10a_exercising_hours.append(0.)\n",
    "\n",
    "    c10b_housework_hours = []\n",
    "    for day in week_context_attributes:\n",
    "        if 'HouseWork' not in week_context_attributes[day].context.values:\n",
    "            c10b_housework_hours.append(0.)\n",
    "        else:\n",
    "            day_housework = week_context_attributes[day][week_context_attributes[day].context == 'HouseWork']\n",
    "            day_housework['period'] = day_housework['end'] - day_housework['start']\n",
    "            c10b_housework_hours.append(day_housework['period'].sum())\n",
    "\n",
    "    if np.min(c10a_exercising_hours) > 15 * 60:\n",
    "        stress_score -=0.5\n",
    "\n",
    "    if np.min(c10b_housework_hours) > 60 * 60:\n",
    "        stress_score -= 0.5\n",
    "\n",
    "\n",
    "\n",
    "    # condition 11a: Continuous 30 mins exercise, five times a week: -1 stress score\n",
    "    # condition 11b: 15 mins exercise twice, five times a week: -1 stress score\n",
    "    # condition 11c: 10 mins exercise thrice, five times a week: -1 stress score\n",
    "\n",
    "    c11_day_cond = []\n",
    "    for day in week_context_attributes.keys():\n",
    "        if 'Exercising' not in week_context_attributes[day].context.values:\n",
    "            c11_day_cond.append(False)\n",
    "        else:\n",
    "            day_exercise_values = week_context_attributes[day][week_context_attributes[day].context == 'OfficeWork']\n",
    "            day_exercise_values['period'] = day_exercise_values['end'] - day_exercise_values['start']\n",
    "            day_num_exercises = day_exercise_values.shape[0]\n",
    "            day_min_continuous_exercise = day_exercise_values['period'].min()\n",
    "            if (day_num_exercises >=1) and (day_min_continuous_exercise > 30*60):\n",
    "                c11_day_cond.append(True)\n",
    "            elif (day_num_exercises >=2) and (day_min_continuous_exercise > 15*60):\n",
    "                c11_day_cond.append(True)\n",
    "            elif (day_num_exercises >=3) and (day_min_continuous_exercise > 10*60):\n",
    "                c11_day_cond.append(True)\n",
    "            else:\n",
    "                c11_day_cond.append(False)\n",
    "\n",
    "    if (np.sum(c11_day_cond)/len(c11_day_cond)) > 0.7: # 5+ days in a week\n",
    "        stress_score -= 1\n",
    "\n",
    "    # condition 12: Working hours more than 8 for more than 5 days: +1 stress score\n",
    "    c12_work_hours = c8_work_hours\n",
    "\n",
    "    if np.percentile(c12_work_hours,0.7) > (8 * 60 * 60):\n",
    "        stress_score += 1\n",
    "\n",
    "    # condition 13a: Inactivity/Amusement for more than 2 hours everyday: -0.5 stress score\n",
    "    # condition 13b: Inactivity/Amusement/Housework for more than 5 hours atleast two days: -0.5 stress score\n",
    "    c13a_inactivity_amusement_hours = []\n",
    "    for day in week_context_attributes:\n",
    "        day_13a_hours = week_context_attributes[day][week_context_attributes[day].context.isin(['Inactivity','Amusement'])]\n",
    "        if day_13a_hours.shape[0] > 0.:\n",
    "            c13a_inactivity_amusement_hours.append((day_13a_hours['end'] - day_13a_hours['start']).sum())\n",
    "        else:\n",
    "            c13a_inactivity_amusement_hours.append(0.)\n",
    "\n",
    "    c13b_inactivity_amusement_housework_hours = []\n",
    "    for day in week_context_attributes:\n",
    "        day_13b_hours = week_context_attributes[day][week_context_attributes[day].context.isin(['Inactivity','Amusement','HouseWork'])]\n",
    "        if day_13b_hours.shape[0] > 0.:\n",
    "            c13b_inactivity_amusement_housework_hours.append((day_13b_hours['end'] - day_13b_hours['start']).sum())\n",
    "        else:\n",
    "            c13b_inactivity_amusement_housework_hours.append(0.)\n",
    "\n",
    "    if np.min(c13a_inactivity_amusement_hours) > (2 * 60 * 60):\n",
    "        stress_score -= 0.5\n",
    "    if np.percentile(c13b_inactivity_amusement_housework_hours, 30) > (5 * 60 * 60):\n",
    "        stress_score -= 0.5\n",
    "\n",
    "    # condition 14a: Working hours between 12am and 8am for any day: +1 stress score\n",
    "    # condition 14b: Working hours between 12am and 8am for more than 4 days: +1 stress score\n",
    "\n",
    "    c14_working_periods = c5_working_periods\n",
    "    c14_night_work = []\n",
    "    for day in c14_working_periods.keys():\n",
    "        if np.sum(c14_working_periods[day][:8]) > 1:\n",
    "            c14_night_work.append(True)\n",
    "        else:\n",
    "            c14_night_work.append(False)\n",
    "\n",
    "    if np.sum(c14_night_work) >=1:\n",
    "        stress_score +=1\n",
    "    if np.sum(c14_night_work) >=4:\n",
    "        stress_score += 1\n",
    "\n",
    "    return stress_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../cache/trace_results/casas_onto_trace.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_onto_casas \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../cache/trace_results/casas_onto_trace.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_onto_casas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtao_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_onto_casas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtao_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m df_onto_casas\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/envs/context_sensing_2/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/context_sensing_2/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/context_sensing_2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/context_sensing_2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/context_sensing_2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/context_sensing_2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/context_sensing_2/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../cache/trace_results/casas_onto_trace.csv'"
     ]
    }
   ],
   "source": [
    "df_onto_casas = pd.read_csv(\"../../cache/trace_results/casas_onto_trace.csv\")\n",
    "df_onto_casas['tao_prediction'] = df_onto_casas['tao_prediction'].apply(lambda x: x.split(\";\") if not (str(x)=='nan') else ['Unknown'])\n",
    "df_onto_casas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ts_dict = pickle.load(open(\"../../cache/trace_results/ts_results_casas.pb\",\"rb\"))\n",
    "ts_results, cluster_labels = ts_dict['ts_results'], ts_dict['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wellness_user_inputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "id = 'csh115'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start id csh115\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 205477 entries, 0 to 205476\n",
      "Data columns (total 4 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   start_timestamp       205477 non-null  float64\n",
      " 1   next_start_timestamp  205477 non-null  float64\n",
      " 2   end_timestamp         205477 non-null  float64\n",
      " 3   cluster_id            205477 non-null  int64  \n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 6.3 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.308097e+09</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.308097e+09</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.308097e+09</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.308097e+09</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.308097e+09</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp     context\n",
       "0  1.308097e+09  [Sleeping]\n",
       "1  1.308097e+09  [Sleeping]\n",
       "2  1.308097e+09  [Sleeping]\n",
       "3  1.308097e+09  [Sleeping]\n",
       "4  1.308097e+09  [Sleeping]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(f\"Start id {id}\")\n",
    "df_ts_input =ts_results[id]\n",
    "df_ts_input.info()\n",
    "pred_min_ts, pred_max_ts = df_ts_input.start_timestamp.min(), df_ts_input.end_timestamp.max()\n",
    "df_pred_ts = pd.DataFrame(np.arange(pred_min_ts, pred_max_ts + 1), columns=['timestamp'])\n",
    "df_pred_ts['pred_context'] = None\n",
    "df_pred_ts = df_pred_ts.set_index('timestamp')\n",
    "for row_idx, row in df_ts_input.iterrows():\n",
    "    df_pred_ts.loc[row['start_timestamp']:row['end_timestamp'], 'pred_context'] = df_pred_ts.loc[\n",
    "                                                                                  row['start_timestamp']:\n",
    "                                                                                  row['end_timestamp'],\n",
    "                                                                                  'pred_context'].apply(\n",
    "        lambda x: (x + cluster_labels[int(row['cluster_id'])]) if (x is not None) else cluster_labels[\n",
    "            int(row['cluster_id'])])\n",
    "df_pred_ts = df_pred_ts.reset_index().rename(columns={'pred_context':'context'})\n",
    "df_pred_ts = df_pred_ts[~df_pred_ts.context.isnull()]\n",
    "df_pred_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3837146</th>\n",
       "      <td>1308096677</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837147</th>\n",
       "      <td>1308096679</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837148</th>\n",
       "      <td>1308096682</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837149</th>\n",
       "      <td>1308096683</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837150</th>\n",
       "      <td>1308096854</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp     context\n",
       "3837146  1308096677  [Sleeping]\n",
       "3837147  1308096679  [Sleeping]\n",
       "3837148  1308096682  [Sleeping]\n",
       "3837149  1308096683  [Sleeping]\n",
       "3837150  1308096854  [Sleeping]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onto_ts = df_onto_casas[df_onto_casas.id==id][['timestamp','tao_prediction']].rename(columns={'tao_prediction':'context'})\n",
    "df_onto_ts = df_onto_ts[~df_onto_ts.context.isnull()]\n",
    "df_onto_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done id csh115\n"
     ]
    }
   ],
   "source": [
    "df_combined_ts = pd.merge(df_onto_ts, df_pred_ts, on=['timestamp'], suffixes=('_onto','_tp'))\n",
    "df_combined_ts.head()\n",
    "df_combined_ts['context'] = df_combined_ts.apply(lambda row: \",\".join(list(row['context_tp'])+list(row['context_onto'])),axis=1)\n",
    "df_combined_ts = df_combined_ts[['timestamp','context']]\n",
    "all_contexts = ['Exercising','HouseWork','Commuting','Sleeping','OfficeWork']\n",
    "for context in all_contexts:\n",
    "    df_combined_ts[context] = df_combined_ts['context'].apply(lambda x: context in x)\n",
    "df_combined_ts = df_combined_ts.drop('context',axis=1)\n",
    "df_combined_ts = pd.melt(df_combined_ts, id_vars='timestamp',var_name='context',value_name='isPresent')\n",
    "df_combined_ts = df_combined_ts[df_combined_ts.isPresent==True][['timestamp','context']]\n",
    "wellness_user_inputs[id] =df_combined_ts\n",
    "print(f\"Done id {id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40442 entries, 0 to 40441\n",
      "Data columns (total 4 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   start_timestamp       40442 non-null  float64\n",
      " 1   next_start_timestamp  40442 non-null  float64\n",
      " 2   end_timestamp         40442 non-null  float64\n",
      " 3   cluster_id            40442 non-null  int64  \n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 1.2 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_context</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.342781e+09</th>\n",
       "      <td>None</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.342781e+09</th>\n",
       "      <td>None</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.342781e+09</th>\n",
       "      <td>None</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.342781e+09</th>\n",
       "      <td>None</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.342781e+09</th>\n",
       "      <td>None</td>\n",
       "      <td>[Sleeping]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred_context     context\n",
       "timestamp                            \n",
       "1.342781e+09         None  [Sleeping]\n",
       "1.342781e+09         None  [Sleeping]\n",
       "1.342781e+09         None  [Sleeping]\n",
       "1.342781e+09         None  [Sleeping]\n",
       "1.342781e+09         None  [Sleeping]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done week 2012_29.\n",
      "Done week 2012_30.\n",
      "Done week 2012_31.\n",
      "Done week 2012_32.\n",
      "Done week 2012_33.\n",
      "Done week 2012_34.\n",
      "Done week 2012_35.\n",
      "Done week 2012_36.\n",
      "Done week 2012_37.\n",
      "Done week 2012_38.\n",
      "Done user csh101\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done week 2011_33.\n",
      "Done user csh102\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done user csh103\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done user csh104\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done week 2011_26.\n",
      "Done user csh105\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done week 2011_33.\n",
      "Done user csh106\n",
      "Done week 2012_29.\n",
      "Done week 2012_30.\n",
      "Done week 2012_31.\n",
      "Done week 2012_32.\n",
      "Done week 2012_33.\n",
      "Done week 2012_34.\n",
      "Done user csh107\n",
      "Done week 2011_35.\n",
      "Done week 2011_36.\n",
      "Done week 2011_37.\n",
      "Done week 2011_38.\n",
      "Done week 2011_39.\n",
      "Done week 2011_40.\n",
      "Done week 2011_41.\n",
      "Done week 2011_42.\n",
      "Done week 2011_43.\n",
      "Done week 2011_44.\n",
      "Done user csh108\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done user csh109\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done user csh110\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done user csh111\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done week 2011_33.\n",
      "Done week 2011_34.\n",
      "Done week 2011_35.\n",
      "Done week 2011_36.\n",
      "Done week 2011_37.\n",
      "Done week 2011_38.\n",
      "Done week 2011_39.\n",
      "Done user csh112\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done week 2011_33.\n",
      "Done week 2011_34.\n",
      "Done week 2011_35.\n",
      "Done week 2011_36.\n",
      "Done week 2011_37.\n",
      "Done week 2011_38.\n",
      "Done week 2011_39.\n",
      "Done week 2011_40.\n",
      "Done week 2011_41.\n",
      "Done week 2011_42.\n",
      "Done week 2011_43.\n",
      "Done week 2011_44.\n",
      "Done week 2011_45.\n",
      "Done week 2011_46.\n",
      "Done week 2011_48.\n",
      "Done week 2011_49.\n",
      "Done week 2011_50.\n",
      "Done week 2011_51.\n",
      "Done week 2011_52.\n",
      "Done week 2012_52.\n",
      "Done week 2012_01.\n",
      "Done week 2012_02.\n",
      "Done week 2012_03.\n",
      "Done week 2012_04.\n",
      "Done week 2012_05.\n",
      "Done week 2012_06.\n",
      "Done week 2012_07.\n",
      "Done week 2012_08.\n",
      "Done week 2012_09.\n",
      "Done week 2012_10.\n",
      "Done week 2012_11.\n",
      "Done week 2012_12.\n",
      "Done week 2012_13.\n",
      "Done week 2012_14.\n",
      "Done week 2012_15.\n",
      "Done week 2012_16.\n",
      "Done week 2012_17.\n",
      "Done week 2012_18.\n",
      "Done week 2012_19.\n",
      "Done week 2012_20.\n",
      "Done week 2012_21.\n",
      "Done week 2012_22.\n",
      "Done week 2012_23.\n",
      "Done week 2012_24.\n",
      "Done week 2012_25.\n",
      "Done week 2012_26.\n",
      "Done week 2012_27.\n",
      "Done week 2012_28.\n",
      "Done week 2012_29.\n",
      "Done week 2012_30.\n",
      "Done week 2012_31.\n",
      "Done week 2012_32.\n",
      "Done week 2012_33.\n",
      "Done week 2012_34.\n",
      "Done week 2012_35.\n",
      "Done week 2012_36.\n",
      "Done week 2012_37.\n",
      "Done week 2012_38.\n",
      "Done week 2012_39.\n",
      "Done week 2012_40.\n",
      "Done week 2012_41.\n",
      "Done week 2012_42.\n",
      "Done week 2012_43.\n",
      "Done week 2012_44.\n",
      "Done week 2012_45.\n",
      "Done user csh113\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done user csh114\n",
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done week 2011_33.\n",
      "Done week 2011_34.\n",
      "Done week 2011_35.\n",
      "Done week 2011_36.\n",
      "Done week 2011_37.\n",
      "Done week 2011_38.\n",
      "Done week 2011_39.\n",
      "Done week 2011_40.\n",
      "Done week 2011_41.\n",
      "Done week 2011_42.\n",
      "Done week 2011_43.\n",
      "Done week 2011_44.\n",
      "Done week 2011_45.\n",
      "Done week 2011_46.\n",
      "Done week 2011_47.\n",
      "Done week 2011_48.\n",
      "Done week 2011_49.\n",
      "Done week 2011_50.\n",
      "Done week 2011_51.\n",
      "Done week 2011_52.\n",
      "Done week 2012_52.\n",
      "Done week 2012_01.\n",
      "Done week 2012_02.\n",
      "Done week 2012_03.\n",
      "Done week 2012_04.\n",
      "Done week 2012_05.\n",
      "Done week 2012_06.\n",
      "Done week 2012_07.\n",
      "Done week 2012_08.\n",
      "Done week 2012_09.\n",
      "Done week 2012_10.\n",
      "Done week 2012_11.\n",
      "Done week 2012_12.\n",
      "Done week 2012_13.\n",
      "Done week 2012_14.\n",
      "Done week 2012_15.\n",
      "Done week 2012_16.\n",
      "Done week 2012_17.\n",
      "Done user csh115\n"
     ]
    }
   ],
   "source": [
    "user_wellness_scores = {}\n",
    "for id in wellness_user_inputs.keys():\n",
    "    df_wellness_input = wellness_user_inputs[id].copy(deep=True)\n",
    "    df_wellness_input['datetime'] = pd.to_datetime(df_wellness_input['timestamp'], unit='s')\n",
    "    df_wellness_input['week'] = df_wellness_input['datetime'].apply(lambda x: x.strftime(\"%Y_%V\"))\n",
    "    df_wellness_input['day'] = df_wellness_input['datetime'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    user_week_context_attributes = []\n",
    "    for week in df_wellness_input['week'].unique():\n",
    "        df_week_wellness_input = df_wellness_input[df_wellness_input.week == week]\n",
    "        week_context_attributes = {}\n",
    "        for day in df_week_wellness_input['day'].unique():\n",
    "            df_day_wellness_input = df_week_wellness_input[df_week_wellness_input.day == day]\n",
    "            df_day_wellness_input['context_grp'] = (\n",
    "                    df_day_wellness_input['context'] != df_day_wellness_input['context'].shift(1)).cumsum()\n",
    "            day_contexts = df_day_wellness_input.groupby(['context_grp', 'context'], as_index=False).agg({\n",
    "                'timestamp': ['min', 'max', lambda x: x.max() - x.min()]\n",
    "            })\n",
    "            day_contexts.columns = ['group', 'context', 'start', 'end', 'length']\n",
    "            week_context_attributes[day] = day_contexts\n",
    "        user_week_context_attributes.append([week, week_context_attributes])\n",
    "        print(f'Done week {week}.')\n",
    "    weekly_stress_scores = []\n",
    "    for week, week_context_attributes in user_week_context_attributes:\n",
    "        week_stress_score = get_weekly_stress_score(week_context_attributes)\n",
    "        weekly_stress_scores.append((week, week_stress_score))\n",
    "    user_wellness_scores[id] = weekly_stress_scores\n",
    "    print(f\"Done user {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_stress =None\n",
    "for id in user_wellness_scores.keys():\n",
    "    df_user_stress = pd.DataFrame(user_wellness_scores[id],columns=['week','score'])\n",
    "    df_user_stress['id'] = id\n",
    "    df_user_stress = df_user_stress[['id','week','score']].sort_values(by='week').reset_index()\n",
    "    df_user_stress.columns =['week_id','id','week','score']\n",
    "    df_user_stress['week_id'] = df_user_stress['week_id'].apply(lambda x: 'week '+ str(x+1))\n",
    "    df_user_stress = df_user_stress.iloc[:5,:]\n",
    "    if df_stress is None:\n",
    "        df_stress = df_user_stress.copy(deep=True)\n",
    "    else:\n",
    "        df_stress = pd.concat([df_stress, df_user_stress],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75 entries, 0 to 74\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   week_id  75 non-null     object \n",
      " 1   id       75 non-null     object \n",
      " 2   week     75 non-null     object \n",
      " 3   score    75 non-null     float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_stress.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_stress.to_csv(\"../../cache/stress_scores.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_wellness_input = df_combined_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_wellness_input['datetime'] = pd.to_datetime(df_wellness_input['timestamp'], unit='s')\n",
    "df_wellness_input['week'] = df_wellness_input['datetime'].apply(lambda x: x.strftime(\"%Y_%V\"))\n",
    "df_wellness_input['day'] = df_wellness_input['datetime'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done week 2011_24.\n",
      "Done week 2011_25.\n",
      "Done week 2011_26.\n",
      "Done week 2011_27.\n",
      "Done week 2011_28.\n",
      "Done week 2011_29.\n",
      "Done week 2011_30.\n",
      "Done week 2011_31.\n",
      "Done week 2011_32.\n",
      "Done week 2011_33.\n"
     ]
    }
   ],
   "source": [
    "user_week_context_attributes = []\n",
    "for week in df_wellness_input['week'].unique():\n",
    "    df_week_wellness_input = df_wellness_input[df_wellness_input.week == week]\n",
    "    week_context_attributes = {}\n",
    "    for day in df_week_wellness_input['day'].unique():\n",
    "        df_day_wellness_input = df_week_wellness_input[df_week_wellness_input.day == day]\n",
    "        df_day_wellness_input['context_grp'] = (\n",
    "                df_day_wellness_input['context'] != df_day_wellness_input['context'].shift(1)).cumsum()\n",
    "        day_contexts = df_day_wellness_input.groupby(['context_grp', 'context'], as_index=False).agg({\n",
    "            'timestamp': ['min', 'max', lambda x: x.max() - x.min()]\n",
    "        })\n",
    "        day_contexts.columns = ['group', 'context', 'start', 'end', 'length']\n",
    "        week_context_attributes[day] = day_contexts\n",
    "    user_week_context_attributes.append([week, week_context_attributes])\n",
    "    print(f'Done week {week}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2011_24', 1.0),\n",
       " ('2011_25', 1.0),\n",
       " ('2011_26', 1.0),\n",
       " ('2011_27', 1.0),\n",
       " ('2011_28', 1.0),\n",
       " ('2011_29', 1.0),\n",
       " ('2011_30', 1.0),\n",
       " ('2011_31', 1.0),\n",
       " ('2011_32', 1.0),\n",
       " ('2011_33', 1.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly_stress_scores = []\n",
    "for week, week_context_attributes in user_week_context_attributes:\n",
    "    week_stress_score = get_weekly_stress_score(user_week_context_attributes[0][1])\n",
    "    weekly_stress_scores.append((week, week_stress_score))\n",
    "weekly_stress_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2012_29',\n",
       " {'2012-07-20':     group        context       start         end  length\n",
       "  0       1       GoingOut  1342780734  1342781454     720\n",
       "  1       3  UsingBathroom  1342782560  1342782599      39\n",
       "  2       5  UsingBathroom  1342782609  1342782609       0\n",
       "  3       6      PhoneCall  1342785004  1342785340     336\n",
       "  4       9  UsingBathroom  1342785645  1342785930     285\n",
       "  5      12       GoingOut  1342786003  1342786018      15\n",
       "  6      13       ComingIn  1342786433  1342786450      17\n",
       "  7      14       Sleeping  1342786485  1342787744    1259\n",
       "  8      16       Sleeping  1342787819  1342789329    1510\n",
       "  9      17        Unknown  1342789491  1342789491       0\n",
       "  10     18       GoingOut  1342789494  1342789509      15\n",
       "  11     19       ComingIn  1342795232  1342795247      15\n",
       "  12     22  UsingBathroom  1342795272  1342795407     135\n",
       "  13     25       Sleeping  1342795479  1342796390     911\n",
       "  14     26        Unknown  1342796446  1342796450       4\n",
       "  15     27     HavingMeal  1342796456  1342796564     108\n",
       "  16     28        Unknown  1342796567  1342796567       0\n",
       "  17     31       Sleeping  1342796579  1342804452    7873\n",
       "  18     33  UsingBathroom  1342804632  1342805202     570\n",
       "  19     37  UsingBathroom  1342806018  1342806055      37\n",
       "  20     40       Relaxing  1342806075  1342807254    1179\n",
       "  21     42       GoingOut  1342807315  1342807333      18\n",
       "  22     44       ComingIn  1342812168  1342812182      14\n",
       "  23     46       ComingIn  1342812184  1342812186       2\n",
       "  24     48       ComingIn  1342812188  1342812188       0\n",
       "  25     50       Relaxing  1342812196  1342817982    5786\n",
       "  26     52       Relaxing  1342818052  1342818052       0\n",
       "  27     53  UsingBathroom  1342818324  1342818452     128\n",
       "  28     55  UsingBathroom  1342818473  1342818607     134\n",
       "  29     56        Unknown  1342818945  1342818945       0\n",
       "  30     57     HavingMeal  1342818946  1342819044      98\n",
       "  31     58       Sleeping  1342819057  1342827881    8824,\n",
       "  '2012-07-21':     group        context       start         end  length\n",
       "  0       1       Sleeping  1342829081  1342831480    2399\n",
       "  1       3  UsingBathroom  1342831513  1342831717     204\n",
       "  2       4       Sleeping  1342850912  1342850919       7\n",
       "  3       5  UsingBathroom  1342850931  1342851090     159\n",
       "  4       7     OfficeWork  1342851352  1342853647    2295\n",
       "  ..    ...            ...         ...         ...     ...\n",
       "  66    102     HavingMeal  1342914068  1342914187     119\n",
       "  67    103  UsingBathroom  1342914335  1342914603     268\n",
       "  68    105  UsingBathroom  1342914610  1342915085     475\n",
       "  69    106        Unknown  1342915095  1342915095       0\n",
       "  70    107       Sleeping  1342915096  1342915198     102\n",
       "  \n",
       "  [71 rows x 5 columns],\n",
       "  '2012-07-22':     group        context       start         end  length\n",
       "  0       1       Sleeping  1342915200  1342926968   11768\n",
       "  1       3        Unknown  1342926983  1342926983       0\n",
       "  2       4  UsingBathroom  1342926984  1342926984       0\n",
       "  3       5        Unknown  1342926986  1342926986       0\n",
       "  4       6  UsingBathroom  1342926991  1342927235     244\n",
       "  ..    ...            ...         ...         ...     ...\n",
       "  63     88        Unknown  1343000796  1343000804       8\n",
       "  64     89     HavingMeal  1343000816  1343000998     182\n",
       "  65     90        Unknown  1343000999  1343001054      55\n",
       "  66     91  UsingBathroom  1343001055  1343001575     520\n",
       "  67     94  UsingBathroom  1343001589  1343001598       9\n",
       "  \n",
       "  [68 rows x 5 columns]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_week_context_attributes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context_sensing_2",
   "language": "python",
   "name": "context_sensing_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
